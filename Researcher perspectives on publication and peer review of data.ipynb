{
 "metadata": {
  "name": "",
  "signature": "sha256:e99149718c339105d90ac100a8373a02546f744c2e6d639b3e3b011be61c8b61"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Researcher perspectives on publication and peer review of data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "John Kratz & Carly Strasser"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Abstract"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Data \"publication\" seeks to appropriate the prestige of authorship in the peer-reviewed literature to reward researchers who create useful and well-documented datasets. \n",
      "The scholarly communication community has embraced data publication as an incentive to document and share data.\n",
      "But, numerous new and ongoing experiments in implementation have not yet resolved what a data publication should be, when data should be peer-reviewed, or how data peer review should work. \n",
      "While researchers have been surveyed extensively regarding data management and sharing, their perceptions and expectations of data publication are largely unknown.\n",
      "To bring this important yet neglected perspective into the conversation, we surveyed ~250 researchers across the sciences and social sciences-- asking what expectations \"data publication\" raises and what features would be useful to evaluate the trustworthiness, evaluate the impact, and enhance the prestige of a data publication.\n",
      "We found that researcher expectations of data publication center on availability, generally through an open database or repository.\n",
      "Few respondents expected published data to be peer-reviewed, but peer-reviewed data enjoyed much greater trust and prestige.\n",
      "The importance of adequate metadata was acknowledged, in that almost all respondents expected data peer review to include evaluation of the data's documentation.\n",
      "Formal citation in the reference list was affirmed by most respondents as the proper way to credit dataset creators.\n",
      "Citation count was viewed as the most useful measure of impact, but download count was seen as nearly as valuable.\n",
      "These results offer practical guidance for data publishers seeking to meet researcher expectations and enhance the value of published data."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Configuration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from IPython.display import display\n",
      "from math import sqrt\n",
      "import matplotlib.pyplot as plt\n",
      "import networkx as nx\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import pylab\n",
      "import scipy as sp\n",
      "import scipy.stats as sps\n",
      "\n",
      "pylab.rcParams['figure.figsize'] = (14.0, 6.0)\n",
      "\n",
      "COLORS = ['#eff3ff', '#bdd7e7', '#6baed6', '#3182bd', '#08519c']\n",
      "BAR_COLOR = '#949494'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def expand_checkbox(checkbox_column, options):\n",
      "    \"\"\"\n",
      "    checkbox_column = a series of lists of boxes checked in response to a question\n",
      "    options = a list the options available to check\n",
      "    \n",
      "    expand checkbox_column into a DataFrame of bools with index= respondents, columns= options\n",
      "    \"\"\"\n",
      "    return pd.DataFrame({option : checkbox_column.apply(lambda x: option in x) for option in options})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Graphing functions"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Bootstrap functions to generate error bars"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bootstrap_percentile_ci(data, n_samples=100000, alpha=0.05, stat_function=np.sum):\n",
      "    \"\"\"\n",
      "    Calculates a confidence interval for True/False count data and returns a tuple (low, high) \n",
      "    \n",
      "    data: (numpy array) of bools to resample\n",
      "    num_samples: (int) number of times to resample\n",
      "    alpha: (float) 1 - desired confidence interval  \n",
      "    \"\"\"\n",
      "    n_responses = len(data)\n",
      "    # get num_samples resampled arrays (of length n) of valid indicies for data\n",
      "    indicies = np.random.randint(0, n_responses, (n_samples, n_responses))\n",
      "    \n",
      "    # generate sorted array of desired stat in each resampled array\n",
      "    stats = [stat_function(data[x]) for x in indicies]\n",
      "    stats.sort()\n",
      "\n",
      "    # return stats at the edge of the 2.5 and 97.5 percentiles\n",
      "    return (stats[int((alpha/2.0)*n_samples)], stats[int((1-alpha/2.0)*n_samples)])\n",
      "\n",
      "\n",
      "def bootstrap_basic_ci(data, n_samples=100000, alpha=0.05, stat_function=np.sum):\n",
      "    \"\"\"\n",
      "    Calculates a confidence interval for True/False count data and returns a tuple (low, high) \n",
      "    \n",
      "    data: (numpy array) of bools to resample\n",
      "    num_samples: (int) number of times to resample\n",
      "    alpha: (float) 1 - desired confidence interval  \n",
      "    \"\"\"\n",
      "    double_observed = 2 * stat_function(data)\n",
      "    high, low = bootstrap_percentile_ci(data, n_samples=n_samples, alpha=alpha, stat_function=stat_function)\n",
      "\n",
      "    return (double_observed - low, double_observed - high)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Graph answers to \"check all that apply\" questions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tuple_normalize(counts, n_responses):\n",
      "    \"\"\"\n",
      "    for a pandas series, convert raw counts to percentages\n",
      "    counts (series): of counts\n",
      "    n_responses (int):\n",
      "    \"\"\"\n",
      "    return map(lambda x: float(x) / n_responses * 100, counts)\n",
      "\n",
      "def interval_to_error(confidence_interval, center):\n",
      "    \"\"\"\n",
      "    confidence_interval (tuple): low, high relative to the origin\n",
      "    center (int, float): measured value (e.g., mean)\n",
      "    returns the ci as a tuple relative to the center (i.e., minus, plus the mean)\n",
      "    \"\"\"\n",
      "    return tuple(map(lambda x: abs(float(x) - center), confidence_interval))\n",
      "    \n",
      "def split_interval(interval):\n",
      "    \"\"\"\n",
      "    split a confidence interval tuple and return as a 2-element Series\n",
      "    \"\"\"\n",
      "    return pd.Series(interval, index= ['low','high'])\n",
      "\n",
      "\n",
      "def graph_checkbox(question, answers):\n",
      "    \"\"\"\n",
      "    graph the answers to a 'check all that apply' question\n",
      "    \n",
      "    \"\"\"\n",
      "    \n",
      "    split_checkbox = responses[question].dropna()\n",
      "\n",
      "    # checkbox_responses== DataFrame of bools where index=individual respondents, columns=answer choices\n",
      "    checkbox_responses = expand_checkbox(split_checkbox, answers)\n",
      "    \n",
      "    # sum checked boxes in each column; response_counts== Series with values=sums, index=answer choices\n",
      "    response_counts = checkbox_responses.sum()\n",
      "    \n",
      "    # resample and sum from each column to bootstrap a confidence interval\n",
      "    # count_confidence_intervals== Series with values= tuples (low, high), index=answer choices\n",
      "    count_confidence_intervals = checkbox_responses.apply(lambda x: bootstrap_basic_ci(np.array(x)))\n",
      "    \n",
      "    #print checkbox_responses.apply(lambda x: bootstrap_percentile_ci(np.array(x))).apply(tuple_normalize, args=([len(checkbox_responses)]))\n",
      "    #print checkbox_responses.apply(lambda x: bootstrap_basic_ci(np.array(x))).apply(tuple_normalize, args=([len(checkbox_responses)]))\n",
      "    \n",
      "    #normalize response_counts to percentage of total respondents to the question and sort\n",
      "    response_counts = response_counts.apply(lambda x: float(x) / len(checkbox_responses) * 100)\n",
      "    response_counts.sort(ascending=True)\n",
      "    \n",
      "    #normalize confidence intervals to percentages\n",
      "    count_confidence_intervals = count_confidence_intervals.apply(tuple_normalize, args=([len(checkbox_responses)]))\n",
      "    \n",
      "    \n",
      "    #convert absolute interval values to distance below and above the observed value\n",
      "    for index in count_confidence_intervals.index.values:\n",
      "        count_confidence_intervals.loc[index] = interval_to_error(count_confidence_intervals.loc[index], \n",
      "                                                                  response_counts.loc[index])\n",
      "    \n",
      "    #split interval tuples into 2 element Series\n",
      "    count_confidence_intervals = count_confidence_intervals.apply(split_interval)\n",
      "\n",
      "\n",
      "    fig = response_counts.plot(kind='barh', color='#08519c', edgecolor='w', grid=False, xlim=(0,100))\n",
      "    fig.errorbar(response_counts.as_matrix()[::-1], np.arange(len(response_counts))[::-1], \n",
      "                 xerr=count_confidence_intervals.T.as_matrix(),\n",
      "                 fmt='none', ecolor=BAR_COLOR, alpha=0.65, elinewidth=1, capsize=8, capthick=2)\n",
      "\n",
      "    fig.tick_params(axis='both', which='both', left='off', top='off', right='off')\n",
      "    fig.spines['top'].set_visible(False) \n",
      "    fig.spines['right'].set_visible(False)\n",
      "    \n",
      "    return fig  #, response_counts\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Graph interrelationships between \"check all that apply\" answers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def graph_fisher_exact(question, answers, alpha=0.05, labels=None):\n",
      "    \n",
      "    checkbox_responses = expand_checkbox(question.dropna(), answers)\n",
      "\n",
      "    fig=nx.Graph()\n",
      "    fig.add_nodes_from(checkbox_responses.columns)\n",
      "    pos = nx.circular_layout(fig, scale=10000)\n",
      "\n",
      "    nx.draw_networkx_nodes(fig, pos, node_size=5000, node_color='#cdcdcd', linewidths=0)\n",
      "    nx.draw_networkx_labels(fig, pos, labels=labels, fontsize=10, font_family='serif')\n",
      "    \n",
      "    #return_columns = pd.MultiIndex.from_product([checkbox_responses.columns, ['OR', 'p-value']])\n",
      "    #return_frame = pd.DataFrame(index=checkbox_responses.columns, columns=return_columns)\n",
      "    \n",
      "    i = 0\n",
      "    for a in checkbox_responses.columns:\n",
      "        i += 1\n",
      "        for b in checkbox_responses.columns[i:]:\n",
      "            square = pd.DataFrame({ 0 : 0, 0 : 0}, index=[True, False], columns=[True, False])        \n",
      "        \n",
      "            # fill in counts\n",
      "            square[True] = (checkbox_responses[checkbox_responses[a] == True][b].value_counts())\n",
      "            square[False] = (checkbox_responses[checkbox_responses[a] == False][b].value_counts())\n",
      "\n",
      "            odds_ratio, p = sps.fisher_exact(square.as_matrix())\n",
      "            \n",
      "            e_color = COLORS[4]\n",
      "            if odds_ratio < 1:\n",
      "                odds_ratio, p = sps.fisher_exact(square.T.as_matrix())\n",
      "                e_color = 'r'\n",
      "            \n",
      "            how_significant = 0.5 if p > alpha else 1.0\n",
      "            \n",
      "            nx.draw_networkx_edges(fig, pos, edgelist=[(a,b)], width=odds_ratio, edge_color=e_color, alpha=how_significant)\n",
      "                     \n",
      "            #return_frame.loc[a,(b,'OR')] = return_frame.loc[b,(a,'OR')] = odds_ratio\n",
      "            #return_frame.loc[a,(b,'p-value')] = return_frame.loc[b,(a,'p-value')]= p\n",
      "    plt.axis('off')\n",
      "    return fig"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Graph answers to Likert-scale (i.e., rate from 1 to 5) questions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def graph_likert(questions, answers, filter_on_column=None, filter_value=None):\n",
      "    \n",
      "    if filter_on_column:\n",
      "        responses_ft = responses[responses[filter_on_column] == filter_value]\n",
      "    else:\n",
      "        responses_ft = responses\n",
      "    \n",
      "    collected_counts = pd.DataFrame(index=answers)\n",
      "    stats = pd.DataFrame(index=questions,columns=['mean', 'ci'])\n",
      "\n",
      "    # set up dict for converstion from likert scale (e.g., 1-5) to 0-100%\n",
      "    number_of_answers = len(answers) \n",
      "    answer_to_value = dict(zip(answers, np.arange(number_of_answers)/float(number_of_answers - 1)*100)) \n",
      "    \n",
      "    for column in questions:\n",
      "        collected_counts[column] = responses_ft[column].value_counts().dropna()\n",
      "       \n",
      "        #scale responses to go from 0 to 100\n",
      "        likert_values = responses[column].dropna().map(answer_to_value)\n",
      "        \n",
      "        #cacluate mean and 95% confidence interval\n",
      "        stats['mean'].loc[column] = likert_values.mean() \n",
      "        stats['ci'].loc[column] = bootstrap_basic_ci(np.array(likert_values), stat_function=np.mean)\n",
      "        \n",
      "    #sort stats and collected_counts by the mean   \n",
      "    stats = stats.sort_index(axis=0, by='mean', ascending=True)\n",
      "    collected_counts = collected_counts.T.reindex(index=stats.index)\n",
      "    collected_counts = collected_counts.div(collected_counts.sum(1).astype(float)/100, axis = 0) \n",
      "    \n",
      "    #convert absolute interval values to distance below and above the observed value\n",
      "    for index in stats.index.values:\n",
      "        stats['ci'].loc[index] = interval_to_error(stats['ci'].loc[index], stats['mean'].loc[index])\n",
      "\n",
      "    #split interval tuples into 2 element Series\n",
      "    stats['ci'] = stats['ci'].apply(split_interval)\n",
      "    \n",
      "    #plot percentages of each response\n",
      "    fig = collected_counts.plot(kind='barh', stacked=True, color=COLORS, grid=False, xlim = (0,100), edgecolor='w') \n",
      "    \n",
      "    # plot mean and 95% confidence interval\n",
      "    fig.plot(stats['mean'], np.arange(len(stats)), marker='o', color='w',axes=fig, markersize=20, markeredgewidth=0, linewidth=0)\n",
      "    fig.errorbar(stats['mean'].as_matrix()[::-1], np.arange(len(stats))[::-1], xerr=stats['ci'],\n",
      "                 fmt='none', ecolor=BAR_COLOR, alpha=0.5, elinewidth=1, capsize=8, capthick=2)\n",
      "    \n",
      "    fig.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=2, ncol=number_of_answers, mode=\"expand\", borderaxespad=0.,frameon=False)\n",
      "    fig.tick_params(axis='both', which='both', left='off', top='off', right='off')\n",
      "    fig.spines['top'].set_visible(False)\n",
      "    fig.spines['right'].set_visible(False)\n",
      "    \n",
      "    return fig"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Read in and filter data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To limit analysis to active researchers, filter out librarians, undergraduates, and respondents who said that they had not generated any data in the last 5 years."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "EXCLUDE = {'role' : 'Librarian', 'discipline' : 'Information science', \n",
      "           'highest_degree' : 'Highschool', 'generated_data' : 'No'}\n",
      "\n",
      "responses = pd.read_csv('DataPubSurvey_anon.csv')\n",
      "\n",
      "for column, value in EXCLUDE.items():\n",
      "    responses = responses[responses[column] != value]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "OSError",
       "evalue": "File b'DataPubSurvey_anon.csv' does not exist",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-7-2aaeae538b1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m            'highest_degree' : 'Highschool', 'generated_data' : 'No'}\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataPubSurvey_anon.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mEXCLUDE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/kratzscience/anaconda/lib/python3.4/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[1;32m    463\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/kratzscience/anaconda/lib/python3.4/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/kratzscience/anaconda/lib/python3.4/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/kratzscience/anaconda/lib/python3.4/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/kratzscience/anaconda/lib/python3.4/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/kratzscience/anaconda/lib/python3.4/site-packages/pandas/parser.so\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3150)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;32m/Users/kratzscience/anaconda/lib/python3.4/site-packages/pandas/parser.so\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:5772)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;31mOSError\u001b[0m: File b'DataPubSurvey_anon.csv' does not exist"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consolidate fine-grained sub-discipline answers into larger disciplines."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DISCIPLINE_MAP = {'Anthropology' : 'Social science',\n",
      "                  'Archaeology' : 'Archaeology',\n",
      "                  'Area studies' : 'Social science',\n",
      "                  'Economics' : 'Social science',\n",
      "                  'Political science' : 'Social science',\n",
      "                  'Psychology' : 'Social science',\n",
      "                  'Sociology' : 'Social science',\n",
      "                  'Astronomy' : 'Space science',\n",
      "                  'Astrophysics' : 'Space science',\n",
      "                  'Environmental Science' : 'Environmental science',\n",
      "                  'Geology' : 'Earth science',\n",
      "                  'Oceanography' : 'Environmental science',\n",
      "                  'Planetary science' : 'Earth science',\n",
      "                  'Biochemistry' : 'Biology',\n",
      "                  'Bioinformatics' : 'Biology',\n",
      "                  'Biology' : 'Biology',\n",
      "                  'Evolutionary Biology' : 'Biology',\n",
      "                  'Neurobiology' : 'Biology',\n",
      "                  'Social science' : 'Social science',\n",
      "                  'Space science' : 'Space science',\n",
      "                  'Earth science' : 'Earth science',\n",
      "                  'Life science' : 'Biology',\n",
      "                  'Chemistry' : 'Physical science',\n",
      "                  'Physics' : 'Physical science',\n",
      "                  'Computer science' : 'Computer science',\n",
      "                  'Mathematics' : 'Mathematics',\n",
      "                  'Information science' : 'Information science',\n",
      "                  'Other' : 'Other'}\n",
      "\n",
      "responses.discipline= responses.discipline.map(DISCIPLINE_MAP).dropna()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Figures and tables"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "**Table 1.** Demographic breakdown of the 249 researchers whose responses are analyzed here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DEMOGRAPHICS = ['discipline', 'highest_degree', 'role','institution']\n",
      "\n",
      "for column in DEMOGRAPHICS:\n",
      "    count = responses[column].value_counts()\n",
      "    percentages = 100 * count.apply(lambda x: float(x) / count.sum())\n",
      "    display(pd.DataFrame([count, percentages], index=['count', 'percent']).T)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 1. Researchers are generally unfamiliar with data-related funder policies."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "AWARENESS_QUESTIONS = ['aware_ostp_policy', 'aware_nsf_dmp', 'aware_nih_data_sharing_policy']\n",
      "AWARENESS_ANSWERS = [\"Never heard of it\", \"Heard of it\", \"Read about it\", \"Know all the details\"]  \n",
      "\n",
      "graph_likert(AWARENESS_QUESTIONS, AWARENESS_ANSWERS, filter_on_column='united_states', filter_value=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Respondents based at US institutions self-reported their familiarity with three government funder policies: NSF Data Management Plan requirements ($n=197$), the NIH data sharing policy (only biologists included, $n=76$), and the Whitehouse OSTP Open Data Initiative ($n=197$). \n",
      "White dots show the mean familiarity for each item; error bars depict bootstrapped 95% confidence intervals."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 2. Researchers primarily share data in response to direct contact (e.g. via email)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Figure 2A."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SHARING_CHANNELS = [\"Email / direct contact\", \"Personal or lab website\", \"Journal website (as supplemental material)\", \n",
      "                    \"Database or repository\"]\n",
      "graph_checkbox('how_shared', SHARING_CHANNELS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Figure 2B."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "graph_checkbox('how_others_got', SHARING_CHANNELS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Figure 2C."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "graph_checkbox('how_you_got', SHARING_CHANNELS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Figure 2D."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HOW_DOCUMENTED_ANSWERS = [\"A traditional research paper based on the data (with analysis and conclusions)\",\n",
      "                          \"A data paper describing the data (without analysis or conclusions)\",\n",
      "                          \"Informal text describing the data\",\n",
      "                          \"Formal metadata describing the data (e.g. as XML)\",\n",
      "                          \"Computer code used to process or generate the data\",\n",
      "                          \"Shared with no additional documentation\"]\n",
      "\n",
      "graph_checkbox('how_documented', HOW_DOCUMENTED_ANSWERS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Researchers primarily share data in response to direct contact (e.g. via email).**\n",
      "Respondents who shared data indicated (A.) the channels they used to share their data, (B.) the channels others used to obtain the data, and (D.) how they documented the data.\n",
      "(C.) Respondents who used others' data indicated the channels through which they obtained the data.\n",
      "Error bars depict bootstrapped 95% confidence intervals."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 3. Formal citation is the preferred method of crediting dataset creators."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Figure 3A."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HOW_CREDITED_ANSWERS = [\"Authorship on paper\",\n",
      "                        \"Acknowledgement in the paper\",\n",
      "                        \"Data cited in the reference list\",\n",
      "                        \"Data cited informally in the text of the paper\"]\n",
      "\n",
      "graph_checkbox('data_sharing_credit', HOW_CREDITED_ANSWERS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Figure 3B."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "graph_checkbox('how_you_credited', HOW_CREDITED_ANSWERS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Respondents indicated (A.) how a dataset creator should be credited, (B.) how they actually credited a dataset creator in the past, and (C.) how satisfied they were with the credit they received the last time someone else published using their data.  (A., B.) Respondents could select more than one item for each question.\n",
      "Error bars depict bootstrapped 95% confidence intervals."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 4. Researcher expectations of data publication center on availability, not peer review."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Figure 4A."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DP_FEATURES = [\"Openly available without contacting the author(s)\",\n",
      "               \"Deposited in a database or repository\",\n",
      "               \"Assigned a unique identifier such as a DOI\",\n",
      "               \"A traditional research paper is based on the data\",\n",
      "               \"A data paper (without conclusions) describes the data\",\n",
      "               \"Packaged with a thorough description of the data\",\n",
      "               \"Packaged with formal metadata describing the data (e.g. as XML)\",\n",
      "               \"Dataset is \\\"peer reviewed\\\"\"]\n",
      "graph_checkbox('publish_definition', DP_FEATURES)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Figure 4B."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PR_FEATURES = [\"Collection and processing methods were evaluated\",\n",
      "               \"Descriptive text is thorough enough to use or replicate the dataset\",\n",
      "               \"Necessary metadata is standardized (e.g. in XML)\",\n",
      "               \"Technical details have been checked (e.g. no missing files no missing values)\",\n",
      "               \"Plausibility considered based on area expertise\",\n",
      "               \"Novelty/impact considered\"]\n",
      "\n",
      "graph_checkbox('peer_review_definition', PR_FEATURES)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Respondents conveyed the expectations raised by the terms (A.) publication and (B.) peer review in the context of data. \n",
      "Respondents could select more than one item for each question. \n",
      "Error bars depict bootstrapped 95% confidence intervals."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 5.  Researchers have coherent expectations of data publication and peer review."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Figure 5A."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dp_labels = {\"Openly available without contacting the author(s)\" : \"openly\\navailable\",\n",
      "             \"Deposited in a database or repository\" : \"repository\\ndeposit\",\n",
      "             \"Assigned a unique identifier such as a DOI\" : \"unique\\nID\",\n",
      "              \"A traditional research paper is based on the data\" : \"traditional\\npaper\",\n",
      "              \"A data paper (without conclusions) describes the data\" : \"data\\npaper\",\n",
      "              \"Packaged with a thorough description of the data\" : \"thorough\\nmetadata\",\n",
      "              \"Packaged with formal metadata describing the data (e.g. as XML)\" : \"formal\\nmetadata\",\n",
      "              \"Dataset is \\\"peer reviewed\\\"\" : \"peer\\nreview\"}\n",
      "graph_fisher_exact(responses.publish_definition, DP_FEATURES, labels=dp_labels, alpha=0.05/28)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Figure 5B."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pr_labels = {\"Collection and processing methods were evaluated\" : \"methods\\nappropriate\",\n",
      "               \"Descriptive text is thorough enough to use or replicate the dataset\" : \"thorough\\nmetadata\",\n",
      "               \"Necessary metadata is standardized (e.g. in XML)\" : \"standard\\nmetadata\",\n",
      "               \"Technical details have been checked (e.g. no missing files no missing values)\" : \"technical\\ndetails\",\n",
      "               \"Plausibility considered based on area expertise\" : \"data\\nplausible\",\n",
      "               \"Novelty/impact considered\" : \"novelty/\\nimpact\"}\n",
      "graph_fisher_exact(responses.peer_review_definition, PR_FEATURES, labels=pr_labels, alpha=0.05/15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Graph of relationships between the researcher expectations shown in Figure 4. \n",
      "Nodes are potential (A.) publication features or (B.) peer review assessments.\n",
      "Edge width is proportional to relationship strength as measured by odds ratio.  \n",
      "Blue edges show positive relationships, red are negative. \n",
      "Dark edges are significant at the $\\alpha=0.05$ level by Fisher's exact test, with correction for multiple hypothesis testing to (A.) $\\alpha=0.0018$ and (B.) $\\alpha=0.0033$.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 6. Researchers trust and value peer review highly."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Figure 6A."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "REVIEW_ACTIONS = [\"reviewed a journal article\",\n",
      "                  \"reviewed a grant proposal\",\n",
      "                  \"reviewed an application to graduate school\",\n",
      "                  \"reviewed a CV to hire someone for your lab\",\n",
      "                  \"served on a hiring committee\",\n",
      "                  \"served on a tenure & promotions committee\"]\n",
      "graph_checkbox('researcher_review_experience', REVIEW_ACTIONS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Figure 6B."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DATA_TRUST = ['traditional_paper_confidence', 'data_paper_confidence', 'peer_review_confidence', 'reuse_confidence']\n",
      "DATA_TRUST_SEQUENCE = [\"No confidence\", \"Little confidence\", \"Some confidence\", \"High confidence\", \"Complete confidence\"]\n",
      "\n",
      "graph_likert(DATA_TRUST,DATA_TRUST_SEQUENCE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Figure 6C."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DATA_IMPACT = ['impact_citation', 'impact_downloads', 'impact_altmetrics', 'impact_google_rank']\n",
      "DATA_IMPACT_SEQUENCE = [\"Not at all useful\", \"Slightly useful\", \"Somewhat useful\", \"Highly useful\", \"Extremely useful\"]\n",
      "\n",
      "graph_likert(DATA_IMPACT, DATA_IMPACT_SEQUENCE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Figure 6D."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PUBLICATION_VALUE = [\"traditional_paper_value\", \"data_paper_pr_value\", \"data_paper_npr_value\", \"dataset_pr_value\", \"dataset_npr_value\"]\n",
      "PUBLICATION_VALUE_SEQUENCE = [\"None\", \"A small amount\", \"Some\", \"Significant\", \"A great deal\"]\n",
      "\n",
      "graph_likert(PUBLICATION_VALUE, PUBLICATION_VALUE_SEQUENCE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(A.) Respondents reported their past experience evaluating other researchers in each context; respondents could select more than one item.\n",
      "Respondents reported (B.) how much trust each data publication feature inspires, (C.) how useful each metric would be for assessing impact, and (D.) how valuable a CV item each kind of data publication would be.\n",
      "White dots show the mean response for each item; error bars depict bootstrapped 95% confidence intervals."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Statistical functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dvar_by_ivar(dvar, dvar_values, ivar):    \n",
      "    \"\"\"\n",
      "    utility to group a likert-scale question by another column \n",
      "    (e.g., group peer review definition by research role)\n",
      "    \"\"\"\n",
      "    checkbox_responses = expand_checkbox(dvar.dropna(), dvar_values)\n",
      "    checkbox_responses[ivar.name] = ivar\n",
      "\n",
      "    # group by ivar values \n",
      "    grouped_by_ivar = checkbox_responses.groupby(ivar.name)\n",
      "    \n",
      "    # create a DataFrame where index= ivar values, columns = multi-index [dvar_values, [True, False]],\n",
      "    # cell values= count for each ivar value, dvar value, and bool value\n",
      "    grouped_frame = grouped_by_ivar.agg([np.count_nonzero, lambda x: np.size(x) - np.count_nonzero(x)])\n",
      "    grouped_frame = grouped_frame.rename(columns={'count_nonzero' : True, '<lambda>' : False})\n",
      "    \n",
      "    return grouped_frame"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Chi square for checkbox questions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def validate_chi_square(expected_values, counts=1):\n",
      "    \"\"\"\n",
      "    check for sufficiently high expected counts in each cell\n",
      "    \"\"\"\n",
      "    for cell in expected_values:\n",
      "        if cell.min() < counts:\n",
      "            print \"expected count less than \" + str(counts)\n",
      "            \n",
      "def chkbx_chi_square(dvar, dvar_values, ivar, validate=True):\n",
      "\n",
      "    grouped_frame = dvar_by_ivar(dvar, dvar_values, ivar)\n",
      "    \n",
      "    # calculate chi square for each dvar value & return all of them in a DataFrame\n",
      "    output = grouped_frame.groupby(level=0, axis=1).apply(lambda x: sps.chi2_contingency(x.as_matrix()))\n",
      "    output = output.apply(lambda x: pd.Series(x, index = ['chi2', 'p', 'df', 'expected_values']))\n",
      "    \n",
      "    if validate: \n",
      "        validate_chi_square(output.expected_values)\n",
      "    \n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Fisher exact test for checkbox questions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def chbx_fisher_exact(dvar, dvar_values, ivar):\n",
      "    grouped_frame = dvar_by_ivar(dvar, dvar_values, ivar)\n",
      "    output = grouped_frame.groupby(level=0, axis=1).apply(lambda x: sps.fisher_exact(x.as_matrix()))\n",
      "    output = output.apply(lambda x: pd.Series(x, index = ['OR', 'p']))\n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def combinatorial_fisher_exact(checkbox_responses):\n",
      "    \"\"\"\n",
      "    checkbox_responses should be a DataFrame in format:\n",
      "                answer1 answer2 ...\n",
      "    respondent1 T/F     T/F\n",
      "    respondent2 T/F     T/F\n",
      "    ...\n",
      "    \n",
      "    returns a DataFrame in format:\n",
      "                option1         option2       ...\n",
      "                OR     p-value  OR    p-value\n",
      "    option1     NaN    NaN      #>1   #\n",
      "    option2     #>1    #        NaN   NaN     ...\n",
      "    ...\n",
      "    \n",
      "    all ORs are calculated to be >1, returned frame is diagonally symmetrical \n",
      "    \"\"\"\n",
      "    \n",
      "    return_columns = pd.MultiIndex.from_product([checkbox_responses.columns, ['OR', 'p-value']])\n",
      "    \n",
      "    return_frame = pd.DataFrame(index=checkbox_responses.columns, columns=return_columns)\n",
      "    \n",
      "    i = 0\n",
      "    for a in checkbox_responses.columns:\n",
      "        i += 1\n",
      "        for b in checkbox_responses.columns[i:]:\n",
      "            square = pd.DataFrame({ 0 : 0, 0 : 0}, index=[True, False], columns=[True, False])        \n",
      "        \n",
      "            # fill in counts\n",
      "            square[True] = (checkbox_responses[checkbox_responses[a] == True][b].value_counts())\n",
      "            square[False] = (checkbox_responses[checkbox_responses[a] == False][b].value_counts())\n",
      "\n",
      "            odds_ratio, p = sps.fisher_exact(square.as_matrix())\n",
      "            \n",
      "            if odds_ratio < 1:\n",
      "                odds_ratio, p = sps.fisher_exact(square.T.as_matrix())\n",
      "                \n",
      "            \n",
      "            return_frame.loc[a,(b,'OR')] = return_frame.loc[b,(a,'OR')] = odds_ratio\n",
      "            return_frame.loc[a,(b,'p-value')] = return_frame.loc[b,(a,'p-value')]= p\n",
      "\n",
      "    return return_frame\n",
      "                    \n",
      "def checkbox_fisher_exact(question, answers):\n",
      "    \n",
      "    # DF of bools; responders x checkbox (checked = True) \n",
      "    checkbox_responses = expand_checkbox(question.dropna(), answers)\n",
      "    \n",
      "    # DF of True and False counts for each checkbox\n",
      "    count_table = checkbox_responses.apply(lambda x: x.value_counts()).T\n",
      "    \n",
      "    return combinatorial_fisher_exact(checkbox_responses)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Jackknife-ish systematic permutation "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def jacknife(data_frame, axis=0):\n",
      "    \"\"\"\n",
      "    returns a list of DataFrames, each with 1 row or column of data_frame deleted\n",
      "    \n",
      "    \"\"\"\n",
      "    frame_list = []\n",
      "    if axis == 0:\n",
      "        for index in data_frame.index:\n",
      "            frame_list.append(data_frame.drop(index))\n",
      "    if axis == 1:\n",
      "        for column in data_frame.columns:\n",
      "            frame_list.append(data_frame.drop(column))\n",
      "    \n",
      "    return frame_list\n",
      "\n",
      "jacknifed_responses = jacknife(responses)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Few differences in attitudes between disciplines were detected."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Statistical significance was tested using Fisher's exact test where possible (i.e., for 2x2 tables) and contingency $\\chi^{2}$ in all other cases.\n",
      "A statistical significance cutoff of $\\alpha=0.05$ was used. \n",
      "When testing for e.g., effects of discipline or prior experience, each answer choice was tested separately, then the Bonferroni correction for multiple hypothesis testing was applied to adjust $\\alpha$ for that question; this is a conservative approach that may not detect subtle differences.\n",
      "Mathematicians were omitted from $\\chi^{2}$  significance testing for effects of discipline because their low $n$ led to unacceptably small expected count numbers.\n",
      "The robustness of all significant results was confirmed using a jackknife procedure in which the test was repeated systematically with each respondent removed; the number of caseses in which the absence of a single respondent moved the p-value accross the designated significance threshold is recorded in the \"changed outcomes\" column."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Not enough mathematicians responded to test reliably.\n",
      "populated_disciplines = responses[responses.discipline != \"Mathematics\"].discipline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha = 0.05/4\n",
      "\n",
      "how_shared_by_discipline = chkbx_chi_square(responses.how_shared, SHARING_CHANNELS, populated_disciplines)\n",
      "\n",
      "how_shared_by_discipline_chi2_frames = []\n",
      "for frame in jacknifed_responses:\n",
      "    how_shared_by_discipline_chi2_frames.append(chkbx_chi_square(frame.how_shared, SHARING_CHANNELS, \n",
      "                                                                 populated_disciplines, validate=False))\n",
      "how_shared_by_discipline['changed outcomes'] = jacknife_effect_1d(how_shared_by_discipline_chi2_frames, \n",
      "                                                                  how_shared_by_discipline, alpha=alpha)\n",
      "\n",
      "display(how_shared_by_discipline)\n",
      "print \"alpha = \" + str(alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha = 0.05/4\n",
      "\n",
      "data_sharing_credit_by_discipline = chkbx_chi_square(responses.data_sharing_credit, HOW_CREDITED_ANSWERS, populated_disciplines)\n",
      "\n",
      "data_sharing_credit_by_discipline_chi2_frames = []\n",
      "for frame in jacknifed_responses:\n",
      "    data_sharing_credit_by_discipline_chi2_frames.append(chkbx_chi_square(frame.data_sharing_credit, HOW_CREDITED_ANSWERS, \n",
      "                                                                          populated_disciplines, validate=False))\n",
      "data_sharing_credit_by_discipline['changed outcomes'] = jacknife_effect_1d(data_sharing_credit_by_discipline_chi2_frames, \n",
      "                                                                           data_sharing_credit_by_discipline, alpha=alpha)\n",
      "\n",
      "display(data_sharing_credit_by_discipline)\n",
      "print \"alpha = \" + str(alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha = 0.05/8\n",
      "\n",
      "publish_definition_by_discipline = chkbx_chi_square(responses.publish_definition, DP_FEATURES, populated_disciplines)\n",
      "\n",
      "publish_definition_by_discipline_chi2_frames = []\n",
      "for frame in jacknifed_responses:\n",
      "    publish_definition_by_discipline_chi2_frames.append(chkbx_chi_square(frame.publish_definition, DP_FEATURES, \n",
      "                                                                         populated_disciplines, validate=False))\n",
      "publish_definition_by_discipline['changed outcomes'] = jacknife_effect_1d(publish_definition_by_discipline_chi2_frames, \n",
      "                                                                          publish_definition_by_discipline, alpha=alpha)\n",
      "display(publish_definition_by_discipline)\n",
      "print \"alpha = \" + str(alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha = 0.05/6\n",
      "\n",
      "peer_review_definition_by_discipline = chkbx_chi_square(responses.peer_review_definition, PR_FEATURES, populated_disciplines)\n",
      "\n",
      "peer_review_definition_by_discipline_chi2_frames = []\n",
      "for frame in jacknifed_responses:\n",
      "    peer_review_definition_by_discipline_chi2_frames.append(chkbx_chi_square(frame.peer_review_definition, PR_FEATURES, \n",
      "                                                                             populated_disciplines, validate=False)) \n",
      "peer_review_definition_by_discipline['changed outcomes'] = jacknife_effect_1d(peer_review_definition_by_discipline_chi2_frames,\n",
      "                                                                              peer_review_definition_by_discipline, alpha=alpha)\n",
      "\n",
      "display(peer_review_definition_by_discipline)\n",
      "print \"alpha = \" + str(alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The only two significant differences among disciplines related to structured metadata: discipline had a significant effect on expectation of formal metadata in the publication process (corrected $\\alpha= 0.006$, $\\chi^{2}= 33.0$, $p= 2.6\\times10^{-5}$) and consideration of standardized metadata in peer review (corrected $\\alpha= 0.008$, $\\chi^{2}= 26.7$, $p= 3.8\\times10^{-4}$). \n",
      "In both cases, the most notable distinctions were the expectations of a large fraction of environmental scientists: 63% (compared to 25% in the population as a whole) for publication and 73% (compared to 39%) for peer review."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Environmental scientists have high expectations for formal metadata."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dp_checkbox = expand_checkbox(responses.publish_definition.dropna(), DP_FEATURES)\n",
      "dp_checkbox['discipline'] = responses.discipline\n",
      "counts = dp_checkbox.groupby('discipline').sum()\n",
      "disc_totals = dp_checkbox.discipline.value_counts()\n",
      "\n",
      "for disc in counts.index:\n",
      "    counts.ix[disc] = counts.ix[disc].apply(lambda x: float(x) / disc_totals[disc])\n",
      "\n",
      "counts[\"Packaged with formal metadata describing the data (e.g. as XML)\"].plot(kind='barh', color=COLORS[4], \n",
      "                                                                               grid='off', ylim=(0,1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pr_checkbox = expand_checkbox(responses.peer_review_definition.dropna(), PR_FEATURES)\n",
      "pr_checkbox['discipline'] = responses.discipline\n",
      "counts = pr_checkbox.groupby('discipline').sum()\n",
      "disc_totals = pr_checkbox.discipline.value_counts()\n",
      "\n",
      "for disc in counts.index:\n",
      "    counts.ix[disc] = counts.ix[disc].apply(lambda x: float(x) / disc_totals[disc])\n",
      "#normalize = counts.apply(test.discipline.value_counts()\n",
      "#display(counts)\n",
      "counts[\"Necessary metadata is standardized (e.g. in XML)\"].plot(kind='bar',color=COLORS[4],\n",
      "                                                                grid='off', ylim=(0,1),\n",
      "                                                                edgecolor='w')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Few differences in attitudes between research roles were detected."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha = 0.05/4\n",
      "\n",
      "how_shared_by_role = chkbx_chi_square(responses.how_shared, SHARING_CHANNELS, responses.role)\n",
      "\n",
      "how_shared_by_role_chi2_frames = []\n",
      "for frame in jacknifed_responses:\n",
      "    how_shared_by_role_chi2_frames.append(chkbx_chi_square(frame.how_shared, SHARING_CHANNELS, \n",
      "                                                           populated_disciplines, validate=False))\n",
      "how_shared_by_role['changed outcomes'] = jacknife_effect_1d(how_shared_by_role_chi2_frames, \n",
      "                                                            how_shared_by_discipline, alpha=alpha)\n",
      "\n",
      "display(how_shared_by_role)\n",
      "print \"alpha = \" + str(alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha = 0.05/4\n",
      "\n",
      "data_sharing_credit_by_role = chkbx_chi_square(responses.data_sharing_credit, HOW_CREDITED_ANSWERS, responses.role)\n",
      "\n",
      "data_sharing_credit_by_role_chi2_frames = []\n",
      "for frame in jacknifed_responses:\n",
      "    data_sharing_credit_by_role_chi2_frames.append(chkbx_chi_square(frame.data_sharing_credit, HOW_CREDITED_ANSWERS, \n",
      "                                                                    populated_disciplines, validate=False))\n",
      "data_sharing_credit_by_role['changed outcomes'] = jacknife_effect_1d(data_sharing_credit_by_role_chi2_frames, \n",
      "                                                                     data_sharing_credit_by_role, alpha=alpha)\n",
      "\n",
      "display(data_sharing_credit_by_role)\n",
      "print \"alpha = \" + str(alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha = 0.05/8\n",
      "\n",
      "publish_definition_by_role = chkbx_chi_square(responses.publish_definition, DP_FEATURES, responses.role)\n",
      "\n",
      "publish_definition_by_role_chi2_frames = []\n",
      "for frame in jacknifed_responses:\n",
      "    publish_definition_by_role_chi2_frames.append(chkbx_chi_square(frame.publish_definition, DP_FEATURES, \n",
      "                                                                   responses.role, validate=False))\n",
      "publish_definition_by_role['changed outcomes'] = jacknife_effect_1d(publish_definition_by_role_chi2_frames, \n",
      "                                                                    publish_definition_by_role, alpha=alpha)\n",
      "\n",
      "display(publish_definition_by_role)\n",
      "print \"alpha = \" + str(alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha = 0.05/6\n",
      "\n",
      "peer_review_definition_by_role = chkbx_chi_square(responses.peer_review_definition, PR_FEATURES, responses.role)\n",
      "\n",
      "peer_review_definition_by_role_chi2_frames = []\n",
      "for frame in jacknifed_responses:\n",
      "    peer_review_definition_by_role_chi2_frames.append(chkbx_chi_square(frame.peer_review_definition, PR_FEATURES, \n",
      "                                                                       responses.role, validate=False))\n",
      "peer_review_definition_by_role['changed outcomes'] = jacknife_effect_1d(peer_review_definition_by_role_chi2_frames, \n",
      "                                                                        peer_review_definition_by_role, alpha=alpha)\n",
      "\n",
      "display(peer_review_definition_by_role)\n",
      "print \"alpha = \" + str(alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def jacknifed_fisher_exact(question, answers):\n",
      "         \n",
      "    checkbox_responses = expand_checkbox(question.dropna(), answers)\n",
      "\n",
      "    jk_responses = jacknife(checkbox_responses)\n",
      "\n",
      "    jk_results = [combinatorial_fisher_exact(frame) for frame in jk_responses]\n",
      "\n",
      "    #jk_results_dict = dict(zip(question.index, jk_results))\n",
      "    #jk_result_panel = pd.Panel(jk_results_dict)\n",
      "    #return jk_result_panel\n",
      "\n",
      "    return jk_results\n",
      "\n",
      "\n",
      "def jacknife_fisher_exact_effect(jacknifed_frames, reference_frame, alpha=0.05, statistic='p-value'):\n",
      "        reference_significance = reference_frame.xs(statistic, level=1, axis=1).apply(lambda x: x < alpha)\n",
      "        \n",
      "        #display(jacknifed_frames)\n",
      "        sig_frames = [frame.xs(statistic, level=1, axis=1).apply(lambda x: x < alpha) for frame in jacknifed_frames]\n",
      "        filter_frames = [frame != reference_significance for frame in sig_frames]\n",
      "        filter_panel = pd.Panel(dict(zip(np.arange(0,len(filter_frames)-1), filter_frames)))\n",
      "        return filter_panel.sum(axis=0)\n",
      "    \n",
      "def jacknife_effect_1d(jacknifed_results_frames, reference_frame, statistic='p', alpha=0.05):\n",
      "        # apply significance threshold to the reference frame\n",
      "        reference_significance = reference_frame.xs(statistic, axis=1).apply(lambda x: x < alpha)\n",
      "        \n",
      "        # apply significance threshold to each jacknifed frame\n",
      "        sig_frames = [frame.xs(statistic, axis=1).apply(lambda x: x < alpha) for frame in jacknifed_results_frames]\n",
      "        \n",
      "        # compare jacknifed frames to reference\n",
      "        filter_frames = [frame != reference_significance for frame in sig_frames]\n",
      "                        \n",
      "        # build panel of filter_frames\n",
      "        filter_results = pd.DataFrame.from_dict(dict(zip(np.arange(0,len(filter_frames)-1), filter_frames)))\n",
      "        \n",
      "        # sum differences for each cell and return\n",
      "        return filter_results.sum(axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The results reported in Figure 4. are at least minimally robust."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Fisher's exact tests used to detect relationships betwen items in Figure 4. were repeated with each respondent removed one-at-a-time.\n",
      "In no case did this change the binary outcome of the test.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "publish_fe = checkbox_fisher_exact(responses.publish_definition, DP_FEATURES)\n",
      "jk_publish_fe = jacknifed_fisher_exact(responses.publish_definition, DP_FEATURES)\n",
      "display(jacknife_fisher_exact_effect(jk_publish_fe, publish_fe, alpha=0.05/28))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reported values show the number of times the p-value changed from significant to insignificant or vice-versa."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pr_fe = checkbox_fisher_exact(responses.peer_review_definition, PR_FEATURES)\n",
      "jk_pr_fe = jacknifed_fisher_exact(responses.peer_review_definition, PR_FEATURES)\n",
      "display(jacknife_fisher_exact_effect(jk_pr_fe, pr_fe, alpha=0.05/15))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reported values show the number of times the p-value changed from significant to insignificant or vice-versa."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}